{"meta":{"title":"ILOSAUR.US","subtitle":"Ideas shared, knowledge spread","description":"Ideas shared, knowledge spread","author":"muhilhams","url":"https://ilosaur.us"},"pages":[{"title":"","date":"2024-06-09T05:54:06.645Z","updated":"2024-06-09T05:54:06.645Z","comments":true,"path":"admin/config.json","permalink":"https://ilosaur.us/admin/config.json","excerpt":"","text":"{\"backend\":{\"name\":\"git-gateway\",\"branch\":\"master\"},\"publish_mode\":\"editorial_workflow\",\"media_folder\":\"source/images\",\"public_folder\":\"/images\",\"collections\":[{\"name\":\"posts\",\"label\":\"Post\",\"folder\":\"source/_posts\",\"create\":true,\"slug\":\"{{slug}}\",\"fields\":[{\"label\":\"Title\",\"name\":\"title\",\"widget\":\"string\"},{\"label\":\"Publish Date\",\"name\":\"date\",\"required\":true,\"widget\":\"datetime\"},{\"label\":\"Edited Date\",\"name\":\"updated\",\"required\":true,\"widget\":\"datetime\"},{\"label\":\"Categories\",\"name\":\"category\",\"required\":true,\"widget\":\"list\"},{\"label\":\"Tags\",\"name\":\"tags\",\"required\":false,\"widget\":\"list\"},{\"label\":\"Keywords\",\"name\":\"keywords\",\"required\":false,\"widget\":\"list\"},{\"label\":\"Body\",\"name\":\"body\",\"widget\":\"markdown\"},{\"label\":\"Display Comments\",\"name\":\"comments\",\"required\":false,\"widget\":\"boolean\",\"default\":true}]}],\"skip_render\":\"admin/*\"}"},{"title":"","date":"2024-06-09T05:54:06.649Z","updated":"2024-06-09T05:54:06.649Z","comments":true,"path":"admin/index.html","permalink":"https://ilosaur.us/admin/index.html","excerpt":"","text":"Content Manager"}],"posts":[{"title":"EFK Stack - Parsing Log with Fluentd","slug":"efk-stack-fluentd-parsing","date":"2020-04-13T09:03:00.000Z","updated":"2020-04-13T09:03:28.000Z","comments":true,"path":"2020/04/13/efk-stack-fluentd-parsing/","permalink":"https://ilosaur.us/2020/04/13/efk-stack-fluentd-parsing/","excerpt":"In this session I will share about something that is quite important in the production environment, namely logging. Logs help us in analyzing and identifying the conditions of an application. The challenge is when we try to identify logs in each application on a different server or location that effort or spend a lot of time. The solution is by applying a Log Server so that the log in each application can be centralized.","text":"In this session I will share about something that is quite important in the production environment, namely logging. Logs help us in analyzing and identifying the conditions of an application. The challenge is when we try to identify logs in each application on a different server or location that effort or spend a lot of time. The solution is by applying a Log Server so that the log in each application can be centralized. The next challenge is how the centralized log is Human readable or can be identified more easily by humans to provide stunning insight or summary. One of them is by parsing raw logs into smaller pieces. In this session I will share how to : Parsing simple raw log Parsing json raw log Parsing raw log xml Parsing multiline log Then the stack is used : CentOS 8 as Operating System. Elasticsearch as log storage. Fluentd as a log aggregation, parser, and agent. Kibana as a visualize tool integrated with Elasticsearch. For details on the EFK stack deploying process, please read the following url https://ilosaur.us/2020/04/11/efk-stack-deploy-efk-stack/ To do a parsing log on fluentd, I use the parser plugin provided by fluentd, including regexp, apache2, apache_error, nginx, syslog, csv, tsv, ltsv, json, msgpack, multiline, none and third party parser grok. Simple parse raw log using fluentd regexp parser.Lets start running python apps to generate dummy logs. 12345678910import datetime, time, random, stringdef randomString(stringLength=10): letters = string.ascii_uppercase return &#x27;&#x27;.join(random.choice(letters) for i in range(stringLength))while True: time.sleep(20) print(datetime.datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)+&quot; - SIMPLEAPPS ORDER-&quot;+randomString(14)+&quot; &quot;+str(random.randrange(10,99,1))) 12345mkdir /var/log/simplepython3 simple-python.py &gt;&gt; /var/log/simple/python-simple.log``` And we can tail this log file [root@efk-fluentd simple]# pwd&#x2F;var&#x2F;log&#x2F;simple[root@efk-fluentd simple]# tree.├── python-simple.log├── python-simple.py0 directories, 4 files[root@efk-fluentd simple]# python3 python-simple.py &gt;&gt; &#x2F;var&#x2F;log&#x2F;simple&#x2F;python-simple.log &amp;[1] 8362[root@efk-fluentd simple]# tail -f &#x2F;var&#x2F;log&#x2F;simple&#x2F;python-simple.log2020-04-13 20:39:48 - SIMPLEAPPS ORDER-NNZWQAIAGQCVJM 152020-04-13 20:40:18 - SIMPLEAPPS ORDER-YTHWQVIKHZUSFE 462020-04-13 20:40:48 - SIMPLEAPPS ORDER-JWTDMRKLONUDNT 542020-04-13 20:41:18 - SIMPLEAPPS ORDER-JKDSUVPFLUTPZW 67 123Edit file `/etc/td-agent/td-agent.conf` , add line `@include config.d/*.conf` for directives in separate configuration files. vi &#x2F;etc&#x2F;td-agent&#x2F;td-agent.confmkdir &#x2F;etc&#x2F;td-agent&#x2F;config.d 123Create config file for parsing string log from file `/var/log/simple/python-simple.log.` vi &#x2F;etc&#x2F;td-agent&#x2F;config.d&#x2F;simple-parsing.confsystemctl restart td-agentsystemctl status td-agent 1 @type tail @id python_simple @type regexp expression (?[^ ]* [^ ]*) - (?[^ ]*) (?[^ ]*) (?[^ ]*) time_key logtime time_format %Y-%m-%d %H:%M:%S types count:integer path /var/log/simple/python-simple.log tag simple_parsing @type stdout 1 #2020-04-13 21:05:49 - SIMPLEAPPS ORDER-EUJPYYAYEOJQKW 92 expression (?[^ ]* [^ ]*) - (?[^ ]*) (?[^ ]*) (?[^ ]*) 1234In config file simple-parsing.log using plugin parser regexp to parse raw log from /var/log/simple/python-simple.log. And in match section using output plugin stdout so logs from file /var/log/simple/python-simple.log will be forwarded to /var/log/td-agent/td-agent.log as stdout logs.Now, just tail file /var/log/td-agent/td-agent. log to check the parsing result. 2020-04-13 21:05:49.000000000 +0700 simple_parsing: {\"apps-name\":\"SIMPLEAPPS\",\"invoice\":\"ORDER-EUJPYYAYEOJQKW\",\"count\":92} 2020-04-13 21:06:19.000000000 +0700 simple_parsing: {\"apps-name\":\"SIMPLEAPPS\",\"invoice\":\"ORDER-KRCSOOQVYGTKMW\",\"count\":68} 2020-04-13 21:07:19.000000000 +0700 simple_parsing: {\"apps-name\":\"SIMPLEAPPS\",\"invoice\":\"ORDER-EXKBWKNDKLXGZS\",\"count\":61} 1 time: 2020-04-13 21:05:49.000000000 +0700 simple_parsing: record: { \"apps-name\":\"SIMPLEAPPS\", \"invoice\":\"ORDER-EUJPYYAYEOJQ\", \"count\":\"92\" } 1234## Simple parse raw json log using fluentd json parser. In this section, we will parsing raw json log using fluentd json parser and output to stdout. Ok lets start with create generator log using simple python script. import datetime, sys, time, random, string def randomString(stringLength&#x3D;10): letters &#x3D; string.ascii_uppercase return ‘’.join(random.choice(letters) for i in range(stringLength)) while True: time.sleep(30) print(‘{“order_time”:”‘+datetime.datetime.now().strftime(“%Y-%m-%d %H:%M:%S”)+’”,”order_id”:”ORDER-‘+randomString(14)+’”,”order_amount”:”‘+str(random.randrange(10,99,1)*1000)+’”}’, flush&#x3D;True)","categories":[{"name":"logging","slug":"logging","permalink":"https://ilosaur.us/categories/logging/"}],"tags":[{"name":"efk","slug":"efk","permalink":"https://ilosaur.us/tags/efk/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://ilosaur.us/tags/elasticsearch/"},{"name":"logging","slug":"logging","permalink":"https://ilosaur.us/tags/logging/"},{"name":"fluentd","slug":"fluentd","permalink":"https://ilosaur.us/tags/fluentd/"},{"name":"parsing","slug":"parsing","permalink":"https://ilosaur.us/tags/parsing/"}],"keywords":[{"name":"logging","slug":"logging","permalink":"https://ilosaur.us/categories/logging/"}]},{"title":"EFK Stack - Deploying EFK Stack (Part1)","slug":"efk-stack-deploy-efk-stack","date":"2020-04-12T09:03:28.084Z","updated":"2020-04-12T09:03:28.092Z","comments":true,"path":"2020/04/12/efk-stack-deploy-efk-stack/","permalink":"https://ilosaur.us/2020/04/12/efk-stack-deploy-efk-stack/","excerpt":"Elasticsearch provides real-time search and analytics for all types of data. Whether you have structured or unstructured text, numerical data, or geospatial data, Elasticsearch can efficiently store and index it in a way that supports fast searches. You can go far beyond simple data retrieval and aggregate information to discover trends and patterns in your data. And as your data and query volume grows, the distributed nature of Elasticsearch enables your deployment to grow seamlessly right along with it.","text":"Elasticsearch provides real-time search and analytics for all types of data. Whether you have structured or unstructured text, numerical data, or geospatial data, Elasticsearch can efficiently store and index it in a way that supports fast searches. You can go far beyond simple data retrieval and aggregate information to discover trends and patterns in your data. And as your data and query volume grows, the distributed nature of Elasticsearch enables your deployment to grow seamlessly right along with it. Elasticsearch is commonly deployed alongside Kibana, a powerful data visualization and dashboard for Elasticsearch. Kibana allows you to explore your Elasticsearch log data through a web interface, and build dashboards and queries to quickly answer questions and gain insight into your Kubernetes applications. EFK Stack Elasticsearch Version 7.6 Fluentd Version 2.6 Kibana Version 7.6 Install ElasticsearchInstall Elasticsearch on node efk-elastic-kibana Update &amp; Upgrade packages1234yum -y updateyum -y upgradeyum -y install epel-releasereboot Set Timezone1timedatectl set-timezone Asia/Jakarta Install &amp; Start Chronyd123yum -y install chronysystemctl enable --now chronydsystemctl status chronyd Install OpenJDK1yum install java-1.8.0-openjdk Add repository Elasticsearch123456789101112131415cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/elasticsearch.repo[elasticsearch-7.x]name=Elasticsearch repository for 7.x packagesbaseurl=https://artifacts.elastic.co/packages/7.x/yumgpgcheck=1gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearchenabled=1autorefresh=1type=rpm-mdEOFrpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearchyum clean allyum makecacheyum repolist Install Elasticsearch1yum -y install elasticsearch Config Elasticsearch, edit file /etc/elasticsearch/elasticsearch.yml1cp /etc/elasticsearch/elasticsearch.yml /etc/elasticsearch/elasticsearch.yml.original 12345678vi /etc/elasticsearch/elasticsearch.ymlnode.name: efk-elastic-kibanadiscovery.zen.minimum_master_nodes: 1network.host: 0.0.0.0http.port: 9200discovery.seed_hosts: [&quot;127.0.0.1&quot;]cluster.initial_master_nodes: [&quot;efk-elastic-kibana&quot;] Enable &amp; Start Elasticsearch Service123systemctl daemon-reloadsystemctl enable --now elasticsearchsystemctl status elasticsearch Verify Elasticsearch123netstat -tulpn | grep 9200curl http://127.0.0.1:9200/_cluster/health?prettycurl http://127.0.0.1:9200/_cat/nodes?v Example Output 123456789101112131415161718192021222324[root@efk-elastic-kibana ~]# curl http://10.199.199.123:9200/_cluster/health?pretty&#123; &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;status&quot; : &quot;green&quot;, &quot;timed_out&quot; : false, &quot;number_of_nodes&quot; : 1, &quot;number_of_data_nodes&quot; : 1, &quot;active_primary_shards&quot; : 6, &quot;active_shards&quot; : 6, &quot;relocating_shards&quot; : 0, &quot;initializing_shards&quot; : 0, &quot;unassigned_shards&quot; : 3, &quot;delayed_unassigned_shards&quot; : 0, &quot;number_of_pending_tasks&quot; : 0, &quot;number_of_in_flight_fetch&quot; : 0, &quot;task_max_waiting_in_queue_millis&quot; : 0, &quot;active_shards_percent_as_number&quot; : 66.66666666666666&#125;[root@efk-elastic-kibana ~]# curl http://10.199.199.123:9200/_cat/indices?vhealth status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open .kibana_task_manager_1 c2A6LOb9QQySpcvfY2r_RQ 1 0 2 1 29.8kb 29.8kbgreen open .apm-agent-configuration EM_3qi6yQOmP9unMRtFJ2g 1 0 0 0 283b 283bgreen open .kibana_1 bsbGNLfDTdqjALyJwBCkIQ 1 0 9 1 44.7kb 44.7kb[root@efk-elastic-kibana ~]# Install KibanaInstall Kibana on node efk-elastic-kibana Install kibana1yum -y install kibana Config Kibana, edit file /etc/kibana/kibana.yml1cp /etc/kibana/kibana.yml /etc/kibana/kibana.yml.original 123vi /etc/kibana/kibana.ymlelasticsearch.hosts: [&quot;http://localhost:9200&quot;] Enable &amp; Start Kibana12systemctl enable --now kibanasystemctl status kibana Install nginx as reverse proxy1yum -y install nginx apache2-utils Generate credential for basic http auth12# kibana passwordecho &quot;kibana:`openssl passwd -apr1`&quot; | sudo tee -a /etc/nginx/htpasswd.kibana Config nginx, edit file /etc/nginx/nginx.conf1cp /etc/nginx/nginx.conf /etc/nginx/nginx.conf.original 123456789101112131415161718192021222324252627282930313233343536373839404142vi /etc/nginx/nginx.confuser nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;include /usr/share/nginx/modules/*.conf;events &#123; worker_connections 1024;&#125;http &#123; log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; include /etc/nginx/conf.d/*.conf; server &#123; listen 80; server_name _; auth_basic &quot;Restricted Access&quot;; auth_basic_user_file /etc/nginx/htpasswd.kibana; location / &#123; proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &#x27;upgrade&#x27;; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; &#125; &#125; &#125; Verify config nginx1nginx -t Enable &amp; Restart nginx123systemctl enable nginx systemctl restart nginxsystemctl status nginx Access kibana via browser Next chapter :Introduction, Install &amp; parsing simple log on fluentdParsing Apps Logs from Docker ContainerEFK Stack Integration with Kubernetes CLuster","categories":[{"name":"logging","slug":"logging","permalink":"https://ilosaur.us/categories/logging/"}],"tags":[{"name":"efk","slug":"efk","permalink":"https://ilosaur.us/tags/efk/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://ilosaur.us/tags/elasticsearch/"},{"name":"logging","slug":"logging","permalink":"https://ilosaur.us/tags/logging/"},{"name":"fluentd","slug":"fluentd","permalink":"https://ilosaur.us/tags/fluentd/"}],"keywords":[{"name":"logging","slug":"logging","permalink":"https://ilosaur.us/categories/logging/"}]},{"title":"Menambahkan Plugin Admin Interface di Hexo","slug":"menambahkan-plugin-admin-interface-di-hexo","date":"2020-04-11T03:18:05.879Z","updated":"2020-04-11T03:18:05.915Z","comments":false,"path":"2020/04/11/menambahkan-plugin-admin-interface-di-hexo/","permalink":"https://ilosaur.us/2020/04/11/menambahkan-plugin-admin-interface-di-hexo/","excerpt":"Hexo admin, merupakan project manajemen tulisan berbasis web (Admin Interface) yang di inisiasi oleh akun Jaredly di Github. Project ini membantu blogger dalam proses penulisan konten yang lebih mudah dan efisien dengan menggunakan browser tanpa harus menggunakan Editor dengen fitur preview markdown dan banyak fitur lainnya seperti deploy konten secara otomatis, membuat kategori baru, membuat halaman baru.","text":"Hexo admin, merupakan project manajemen tulisan berbasis web (Admin Interface) yang di inisiasi oleh akun Jaredly di Github. Project ini membantu blogger dalam proses penulisan konten yang lebih mudah dan efisien dengan menggunakan browser tanpa harus menggunakan Editor dengen fitur preview markdown dan banyak fitur lainnya seperti deploy konten secara otomatis, membuat kategori baru, membuat halaman baru. Plugin ini terinspirasi dari svbtle dan prose.io. Untuk Hexo versi 2.x dapat menggunakan Hexo Admin versi 0.3.0 dan khusus untuk Hexo versi 3.x ke atas dapat menggunakan Hexo Admin 1.x. Plugin hexo-admin ini di khususkan untuk editor lokal saja, atau untuk menulis sebuah konten di lokal yang nantinya akan di deploy ke git (hexo deploy) baik menggunakan Github Pages atau Static Server lainnya. Lalu bagaimana cara memasang plugin ini? Sebelumnya, dimulai dari tahap memasang node.js, npm, dan Hexo Site Static Generator. 123456sudo apt install nodejs npmnpm install -g hexocd ~/hexo init blogsayacd blogsayanpm install Setelah itu lanjut dengan memasang plugin hexo-admin 12npm install --save hexo-adminhexo server -d Untuk verifikasi dan akses hexo-admin defaultnya dapat mengakses http://localhost:4000/admin/ . Kemudian untuk mengaktifkan autentikasi ke hexo-admin dengan menambah beberapa baris script di file _config.yml yang dapat di generate secara otomatis di menu Settings -&gt; Setup authentification here. Seperti ilustrasi di bawah ini. potongan file _config.yml bagian autentikasi 1234567. . . . .# hexo-admin authentificationadmin: username: username password_hash: $2a$10$L.XAIqIWgTc5S1zpvV3MEu7/rH34p4Is/nq824smv8EZ3lIPCp1su secret: my super secret phrase. . . . . Beberapa ilustrasi hexo-admin : Untuk fitur deploy otomatis dapat merujuk ke issue Deploy Command Issue . Sekian, semoga bermanfaat! MIS **https://hexo.io/**https://github.com/jaredly/hexo-admin/","categories":[{"name":"blog","slug":"blog","permalink":"https://ilosaur.us/categories/blog/"}],"tags":[],"keywords":[{"name":"blog","slug":"blog","permalink":"https://ilosaur.us/categories/blog/"}]}]}