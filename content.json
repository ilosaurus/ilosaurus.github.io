{"meta":{"title":"ILOSAUR.US","subtitle":"Ideas shared, knowledge spread","description":"Ideas shared, knowledge spread","author":"muhilhams","url":"https://ilosaur.us"},"pages":[{"title":"","date":"2024-06-09T05:54:06.645Z","updated":"2024-06-09T05:54:06.645Z","comments":true,"path":"admin/config.json","permalink":"https://ilosaur.us/admin/config.json","excerpt":"","text":"{\"backend\":{\"name\":\"git-gateway\",\"branch\":\"master\"},\"publish_mode\":\"editorial_workflow\",\"media_folder\":\"source/images\",\"public_folder\":\"/images\",\"collections\":[{\"name\":\"posts\",\"label\":\"Post\",\"folder\":\"source/_posts\",\"create\":true,\"slug\":\"{{slug}}\",\"fields\":[{\"label\":\"Title\",\"name\":\"title\",\"widget\":\"string\"},{\"label\":\"Publish Date\",\"name\":\"date\",\"required\":true,\"widget\":\"datetime\"},{\"label\":\"Edited Date\",\"name\":\"updated\",\"required\":true,\"widget\":\"datetime\"},{\"label\":\"Categories\",\"name\":\"category\",\"required\":true,\"widget\":\"list\"},{\"label\":\"Tags\",\"name\":\"tags\",\"required\":false,\"widget\":\"list\"},{\"label\":\"Keywords\",\"name\":\"keywords\",\"required\":false,\"widget\":\"list\"},{\"label\":\"Body\",\"name\":\"body\",\"widget\":\"markdown\"},{\"label\":\"Display Comments\",\"name\":\"comments\",\"required\":false,\"widget\":\"boolean\",\"default\":true}]}],\"skip_render\":\"admin/*\"}"},{"title":"","date":"2024-06-09T05:54:06.649Z","updated":"2024-06-09T05:54:06.649Z","comments":true,"path":"admin/index.html","permalink":"https://ilosaur.us/admin/index.html","excerpt":"","text":"Content Manager"}],"posts":[{"title":"PostgreSQL HA with Repmgr and Keepalived","slug":"PostgreSQL-HA-with-Repmgr-and-Keepalived","date":"2024-06-09T06:43:00.000Z","updated":"2024-06-10T09:44:19.394Z","comments":true,"path":"2024/06/09/PostgreSQL-HA-with-Repmgr-and-Keepalived/","permalink":"https://ilosaur.us/2024/06/09/PostgreSQL-HA-with-Repmgr-and-Keepalived/","excerpt":"PostgreSQL HA with repmgr and keepalived (pg_rewind)PostgreSQLPostgreSQL is a powerful, open-source relational database management system (RDBMS) that has earned a reputation for reliability, feature robustness, and performance. It supports a wide range of data types and advanced data handling capabilities, including transactions, foreign keys, views, triggers, and stored procedures. PostgreSQL is designed to handle a variety of workloads, from single-machine applications to web services with many concurrent users.","text":"PostgreSQL HA with repmgr and keepalived (pg_rewind)PostgreSQLPostgreSQL is a powerful, open-source relational database management system (RDBMS) that has earned a reputation for reliability, feature robustness, and performance. It supports a wide range of data types and advanced data handling capabilities, including transactions, foreign keys, views, triggers, and stored procedures. PostgreSQL is designed to handle a variety of workloads, from single-machine applications to web services with many concurrent users. High Availability (HA) in PostgreSQL ensures that the database system remains operational and accessible even in the event of hardware or software failures. HA setups typically involve multiple database servers configured in such a way that if the primary server goes down, one of the standby servers can take over with minimal downtime. This is crucial for applications requiring continuous uptime and data reliability. PostgreSQL HA with keepalived and repmgrImplementing HA in PostgreSQL can be achieved using various tools and techniques. Two critical components often used are: repmgr: A suite of open-source tools to manage replication and failover within a cluster of PostgreSQL servers. keepalived: A routing software written in C. It provides simple and robust facilities for load balancing and high-availability. keepalived uses the Virtual Router Redundancy Protocol (VRRP) to ensure that a high-availability cluster of PostgreSQL servers can continue to serve clients seamlessly even if the master server fails. HLD Recovery Scenario A common problem in HA setups is the \"split-brain\" scenario, where two nodes believe they are both the primary, leading to data inconsistency. This can be prevented using `pg_rewind`, a PostgreSQL tool that synchronizes a failed primary with the new primary, ensuring data consistency before it rejoins as a standby. Step-by-Step Configuration and TestingThis guide walks you through setting up a PostgreSQL HA cluster with repmgr and keepalived, and demonstrates how to test it from an application node. Cluster ConfigurationStep 1: Setup PostgreSQL on all nodesExecute on both nodeInstall PostgreSQL on all nodes (ilham-node1, ilham-node2, and ilham-node3). 12sudo apt-get updatesudo apt-get install postgresql postgresql-contrib Step 2: Configure PostgreSQL for ReplicationExecute on both nodeOn all nodes, edit the postgresql.conf file to enable replication and configure necessary settings. 12345678910111213shared_buffers = 2GBmax_connections = 200listen_addresses = &#x27;*&#x27;wal_level = &#x27;replica&#x27;wal_log_hints = onarchive_mode = onarchive_command = &#x27;cp %p /var/lib/postgresql/16/main/archive/%f&#x27;max_wal_senders = 10max_replication_slots = 10wal_keep_size = 64MBhot_standby = onshared_preload_libraries = &#x27;repmgr&#x27; Update the pg_hba.conf file to allow replication connections. 1vim /var/lib/postgresql/16/main/pg_hba.conf 123456789101112131415161718192021# TYPE DATABASE USER ADDRESS METHOD# &quot;local&quot; is for Unix domain socket connections onlylocal all all trust# IPv4 local connections:host all all 127.0.0.1/32 scram-sha-256# IPv6 local connections:host all all ::1/128 scram-sha-256# Allow replication connections from localhost, by a user with the# replication privilege.local replication all trusthost replication repmgr 0.0.0.0/0 trusthost replication repmgr 192.168.0.0/24 md5host replication repmgr 192.168.0.21/24 md5host replication repmgr 192.168.0.22/24 md5host all all 0.0.0.0/0 trusthost repmgr repmgr 192.168.0.0/24 md5host repmgr repmgr 192.168.0.21/24 md5host repmgr repmgr 192.168.0.22/24 md5host replication all 127.0.0.1/32 scram-sha-256host replication all ::1/128 scram-sha-256 Setup postgres user 1vim /etc/sudoers.d/postgres 12345postgres ALL=(ALL) NOPASSWD: ALLpostgres ALL=(ALL) NOPASSWD: /bin/systemctl start postgresql.servicepostgres ALL=(ALL) NOPASSWD: /bin/systemctl stop postgresql.servicepostgres ALL=(ALL) NOPASSWD: /bin/systemctl restart postgresql.servicepostgres ALL=(ALL) NOPASSWD: /bin/systemctl reload postgresql.service Restart PostgreSQL server on both node 1234sudo systemctl enable postgresqlsudo systemctl restart postgresqlsudo systemctl status postgresqlsudo ss -nlptu | grep postgres Step 3: Initialize the Primary Node (ilham-node1)Execute on primary node (ilham-node1)Create the repmgr user and database on Primary 123mkdir -p /etc/repmgr/16/sudo touch /etc/repmgr/16/repmgr.confchown -R postgres:postgres /etc/repmgr/16/ 1sudo vim /etc/repmgr/16/repmgr.conf 1234567891011121314151617181920212223node_id=1node_name=&#x27;ilham-node1&#x27;conninfo=&#x27;host=192.168.0.21 user=repmgr dbname=repmgr connect_timeout=2&#x27;data_directory=&#x27;/var/lib/postgresql/16/main&#x27;use_replication_slots=1log_level=INFOlog_file=&#x27;/var/log/repmgr/repmgr.log&#x27;pg_bindir=&#x27;/usr/lib/postgresql/16/bin&#x27;promote_command=&#x27;/usr/local/bin/promote.sh&#x27;follow_command=&#x27;/usr/local/bin/follow.sh&#x27;standby_disconnect_on_failover=truefailover=&#x27;automatic&#x27;reconnect_attempts=6reconnect_interval=10monitor_interval_secs=2node_rejoin_timeout=30standby_reconnect_timeout=30log_status_interval=20ssh_options=&#x27;-o &quot;StrictHostKeyChecking=no&quot;&#x27;service_start_command=&#x27;sudo /usr/bin/systemctl start postgresql&#x27;service_stop_command=&#x27;sudo /usr/bin/systemctl stop postgresql&#x27;service_restart_command=&#x27;sudo /usr/bin/systemctl restart postgresql&#x27;service_reload_command=&#x27;sudo /usr/bin/systemctl reload postgresql&#x27; Create a replication user and setup the primary database. 12345sudo -u postgres createuser --superuser repmgrsudo -u postgres createdb --owner=repmgr repmgrsudo -u postgres psql -c &quot;ALTER USER repmgr SET search_path TO repmgr, public;&quot;sudo -u postgres psql -c &quot;\\l&quot;sudo -u postgres psql -c &quot;\\du&quot; Test connection from standby node: 12su - postgrespsql &quot;host=192.168.0.21 user=repmgr dbname=repmgr connect_timeout=2&quot; Setup follow and promote script 123456789101112131415161718sudo echo &#x27;&#x27; &gt; /usr/local/bin/promote.shsudo echo &#x27;&#x27; &gt; /usr/local/bin/follow.shsudo echo &#x27;&#x27; &gt; /var/log/repmgr/follow.logsudo echo &#x27;&#x27; &gt; /var/log/repmgr/promote.logchmod 755 /usr/local/bin/promote.shchmod 755 /usr/local/bin/follow.shchown root:postgres /usr/local/bin/promote.shchown root:postgres /usr/local/bin/follow.shchown postgres:postgres /var/log/repmgr/follow.logchown postgres:postgres /var/log/repmgr/promote.logls -l /usr/local/bin/promote.shls -l /usr/local/bin/follow.shls -l /var/log/repmgr/follow.logls -l /var/log/repmgr/promote.log Setup promote script 1vim /usr/local/bin/promote.sh 12345678910111213141516171819202122232425#!/bin/bash# Logging functionlog() &#123; echo &quot;$(date +&#x27;%Y-%m-%d %H:%M:%S&#x27;) - $1&quot; | tee -a /var/log/repmgr/promote.log&#125;log &quot;Starting promotion...&quot;log &quot;Stopping repmgrd service...&quot;sudo systemctl stop repmgrdlog &quot;Promoting the standby to primary...&quot;sudo -u postgres repmgr standby promote -f /etc/repmgr/16/repmgr.conf --log-to-filesleep 2log &quot;Starting repmgrd service...&quot;sudo systemctl start repmgrdsleep 2# Check if promotion was successfulif [ $? -eq 0 ]; then echo &quot;Node successfully promoted to primary.&quot; log &quot;Node successfully promoted to primary.&quot;else echo &quot;Failed to promote node to primary.&quot; log &quot;Failed to promote node to primary.&quot; exit 1fi Setup follow script 1vim /usr/local/bin/follow.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105#!/bin/bash# ConfigurationREPMGR_CONFIG=&quot;/etc/repmgr/16/repmgr.conf&quot;PRIMARY_CONNINFO=&quot;host=192.168.0.22 dbname=repmgr user=repmgr&quot;LOGFILE=&quot;/var/log/repmgr/follow.log&quot;MAX_WAIT=30RETRY_INTERVAL=5RETRY_COUNT=5# Logging functionlog() &#123; echo &quot;$(date +&#x27;%Y-%m-%d %H:%M:%S&#x27;) - $1&quot; | tee -a $LOGFILE&#125;log &quot;Checking PostgreSQL status...&quot;# Check if the node is in recovery mode (standby)IS_STANDBY=$(sudo -u postgres psql -h 192.168.0.21 -U repmgr -d repmgr -tAc &quot;SELECT pg_is_in_recovery()&quot;)if [ &quot;$IS_STANDBY&quot; == &quot;t&quot; ]; then log &quot;Node is already standby, running follow command.&quot; # Check if the replication slot exists before attempting to drop it SLOT_EXISTS=$(sudo -u postgres psql -tAc &quot;SELECT 1 FROM pg_replication_slots WHERE slot_name = &#x27;repmgr_slot_1&#x27;&quot;) if [ &quot;$SLOT_EXISTS&quot; == &quot;1&quot; ]; then log &quot;Dropping existing replication slot &#x27;repmgr_slot_1&#x27;...&quot; sudo -u postgres psql -c &quot;SELECT pg_drop_replication_slot(&#x27;repmgr_slot_1&#x27;)&quot; 2&gt;/dev/null else log &quot;Replication slot &#x27;repmgr_slot_1&#x27; does not exist.&quot; fi sudo -u postgres repmgr standby follow -f $REPMGR_CONFIG --log-to-fileelse log &quot;Node is primary, checking if it should be rejoined as standby.&quot; # Check if the node is registered as standby but running as primary NODE_STATUS=$(sudo -u postgres repmgr -f $REPMGR_CONFIG cluster show | grep -E &#x27;running as primary&#x27; | grep -E &#x27;standby&#x27;) if [ -n &quot;$NODE_STATUS&quot; ]; then log &quot;Node is registered as a standby but running as primary. Correcting this state.&quot; log &quot;Stopping PostgreSQL service...&quot; sudo systemctl stop postgresql # Wait for PostgreSQL service to stop WAIT_TIME=0 while systemctl is-active --quiet postgresql; do sleep 1 WAIT_TIME=$((WAIT_TIME+1)) if [ $WAIT_TIME -ge $MAX_WAIT ]; then log &quot;PostgreSQL service did not stop within $MAX_WAIT seconds. Exiting rejoin process.&quot; exit 1 fi done log &quot;PostgreSQL service stopped successfully.&quot; sleep 6 # Retry logic for repmgr node rejoin RETRY=0 while [ $RETRY -lt $RETRY_COUNT ]; do log &quot;Running repmgr node rejoin (attempt $((RETRY+1))/$RETRY_COUNT)...&quot; sudo -u postgres repmgr node rejoin -f $REPMGR_CONFIG -d &quot;$PRIMARY_CONNINFO&quot; --force-rewind --config-files=postgresql.local.conf,postgresql.conf --verbose if [ $? -eq 0 ]; then log &quot;Rejoin successful.&quot; break else log &quot;Rejoin failed, retrying in $RETRY_INTERVAL seconds...&quot; sleep $RETRY_INTERVAL RETRY=$((RETRY+1)) fi done if [ $RETRY -eq $RETRY_COUNT ]; then log &quot;Rejoin failed after $RETRY_COUNT attempts. Forcing PostgreSQL to start.&quot; sudo systemctl start postgresql sleep 6 else log &quot;Starting PostgreSQL service...&quot; sudo systemctl start postgresql sleep 6 fi log &quot;Starting repmgrd service...&quot; sudo systemctl restart repmgrd log &quot;Follow script completed.&quot; else log &quot;Node is primary, but no mismatch in status detected. No action required.&quot; fifi Step 4: Initialize the Standby Node (ilham-node2)Execute on standby node (ilham-node2)Create the repmgr user and database on Primary 123mkdir -p /etc/repmgr/16/sudo touch /etc/repmgr/16/repmgr.confchown -R postgres:postgres /etc/repmgr/16/ 1sudo vim /etc/repmgr/16/repmgr.conf 1234567891011121314151617181920212223node_id=2node_name=&#x27;ilham-node2&#x27;conninfo=&#x27;host=192.168.0.19 user=repmgr dbname=repmgr connect_timeout=2&#x27;data_directory=&#x27;/var/lib/postgresql/16/main&#x27;use_replication_slots=1log_level=INFOlog_file=&#x27;/var/log/repmgr/repmgr.log&#x27;pg_bindir=&#x27;/usr/lib/postgresql/16/bin&#x27;promote_command=&#x27;repmgr standby promote -f /etc/repmgr/16/repmgr.conf&#x27;follow_command=&#x27;repmgr standby follow -f /etc/repmgr/16/repmgr.conf --upstream-node-id=%n&#x27;standby_disconnect_on_failover=truefailover=&#x27;automatic&#x27;reconnect_attempts=6reconnect_interval=10monitor_interval_secs=2node_rejoin_timeout=30standby_reconnect_timeout=30log_status_interval=20ssh_options=&#x27;-o &quot;StrictHostKeyChecking=no&quot;&#x27;service_start_command=&#x27;sudo /usr/bin/systemctl start postgresql&#x27;service_stop_command=&#x27;sudo /usr/bin/systemctl stop postgresql&#x27;service_restart_command=&#x27;sudo /usr/bin/systemctl restart postgresql&#x27;service_reload_command=&#x27;sudo /usr/bin/systemctl reload postgresql&#x27; Create a replication user and setup the primary database. 12345sudo -u postgres createuser --superuser repmgrsudo -u postgres createdb --owner=repmgr repmgrsudo -u postgres psql -c &quot;ALTER USER repmgr SET search_path TO repmgr, public;&quot;sudo -u postgres psql -c &quot;\\l&quot;sudo -u postgres psql -c &quot;\\du&quot; Test connection from primary node: 12su - postgrespsql &quot;host=192.168.0.22 user=repmgr dbname=repmgr connect_timeout=2&quot; Setup follow and promote script 123456789101112131415161718sudo echo &#x27;&#x27; &gt; /usr/local/bin/promote.shsudo echo &#x27;&#x27; &gt; /usr/local/bin/follow.shsudo echo &#x27;&#x27; &gt; /var/log/repmgr/follow.logsudo echo &#x27;&#x27; &gt; /var/log/repmgr/promote.logchmod 755 /usr/local/bin/promote.shchmod 755 /usr/local/bin/follow.shchown root:postgres /usr/local/bin/promote.shchown root:postgres /usr/local/bin/follow.shchown postgres:postgres /var/log/repmgr/follow.logchown postgres:postgres /var/log/repmgr/promote.logls -l /usr/local/bin/promote.shls -l /usr/local/bin/follow.shls -l /var/log/repmgr/follow.logls -l /var/log/repmgr/promote.log Setup promote script 1vim /usr/local/bin/promote.sh 12345678910111213141516171819202122232425#!/bin/bash# Logging functionlog() &#123; echo &quot;$(date +&#x27;%Y-%m-%d %H:%M:%S&#x27;) - $1&quot; | tee -a /var/log/repmgr/promote.log&#125;log &quot;Starting promotion...&quot;log &quot;Stopping repmgrd service...&quot;sudo systemctl stop repmgrdlog &quot;Promoting the standby to primary...&quot;sudo -u postgres repmgr standby promote -f /etc/repmgr/16/repmgr.conf --log-to-filesleep 2log &quot;Starting repmgrd service...&quot;sudo systemctl start repmgrdsleep 2# Check if promotion was successfulif [ $? -eq 0 ]; then echo &quot;Node successfully promoted to primary.&quot; log &quot;Node successfully promoted to primary.&quot;else echo &quot;Failed to promote node to primary.&quot; log &quot;Failed to promote node to primary.&quot; exit 1fi Setup follow script 1vim /usr/local/bin/follow.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105#!/bin/bash# ConfigurationREPMGR_CONFIG=&quot;/etc/repmgr/16/repmgr.conf&quot;PRIMARY_CONNINFO=&quot;host=192.168.0.21 dbname=repmgr user=repmgr&quot;LOGFILE=&quot;/var/log/repmgr/follow.log&quot;MAX_WAIT=30RETRY_INTERVAL=5RETRY_COUNT=5# Logging functionlog() &#123; echo &quot;$(date +&#x27;%Y-%m-%d %H:%M:%S&#x27;) - $1&quot; | tee -a $LOGFILE&#125;log &quot;Checking PostgreSQL status...&quot;# Check if the node is in recovery mode (standby)IS_STANDBY=$(sudo -u postgres psql -h 192.168.0.22 -U repmgr -d repmgr -tAc &quot;SELECT pg_is_in_recovery()&quot;)if [ &quot;$IS_STANDBY&quot; == &quot;t&quot; ]; then log &quot;Node is already standby, running follow command.&quot; # Check if the replication slot exists before attempting to drop it SLOT_EXISTS=$(sudo -u postgres psql -tAc &quot;SELECT 1 FROM pg_replication_slots WHERE slot_name = &#x27;repmgr_slot_1&#x27;&quot;) if [ &quot;$SLOT_EXISTS&quot; == &quot;1&quot; ]; then log &quot;Dropping existing replication slot &#x27;repmgr_slot_1&#x27;...&quot; sudo -u postgres psql -c &quot;SELECT pg_drop_replication_slot(&#x27;repmgr_slot_1&#x27;)&quot; 2&gt;/dev/null else log &quot;Replication slot &#x27;repmgr_slot_1&#x27; does not exist.&quot; fi sudo -u postgres repmgr standby follow -f $REPMGR_CONFIG --log-to-fileelse log &quot;Node is primary, checking if it should be rejoined as standby.&quot; # Check if the node is registered as standby but running as primary NODE_STATUS=$(sudo -u postgres repmgr -f $REPMGR_CONFIG cluster show | grep -E &#x27;running as primary&#x27; | grep -E &#x27;standby&#x27;) if [ -n &quot;$NODE_STATUS&quot; ]; then log &quot;Node is registered as a standby but running as primary. Correcting this state.&quot; log &quot;Stopping PostgreSQL service...&quot; sudo systemctl stop postgresql # Wait for PostgreSQL service to stop WAIT_TIME=0 while systemctl is-active --quiet postgresql; do sleep 1 WAIT_TIME=$((WAIT_TIME+1)) if [ $WAIT_TIME -ge $MAX_WAIT ]; then log &quot;PostgreSQL service did not stop within $MAX_WAIT seconds. Exiting rejoin process.&quot; exit 1 fi done log &quot;PostgreSQL service stopped successfully.&quot; sleep 6 # Retry logic for repmgr node rejoin RETRY=0 while [ $RETRY -lt $RETRY_COUNT ]; do log &quot;Running repmgr node rejoin (attempt $((RETRY+1))/$RETRY_COUNT)...&quot; sudo -u postgres repmgr node rejoin -f $REPMGR_CONFIG -d &quot;$PRIMARY_CONNINFO&quot; --force-rewind --config-files=postgresql.local.conf,postgresql.conf --verbose if [ $? -eq 0 ]; then log &quot;Rejoin successful.&quot; break else log &quot;Rejoin failed, retrying in $RETRY_INTERVAL seconds...&quot; sleep $RETRY_INTERVAL RETRY=$((RETRY+1)) fi done if [ $RETRY -eq $RETRY_COUNT ]; then log &quot;Rejoin failed after $RETRY_COUNT attempts. Forcing PostgreSQL to start.&quot; sudo systemctl start postgresql sleep 6 else log &quot;Starting PostgreSQL service...&quot; sudo systemctl start postgresql sleep 6 fi log &quot;Starting repmgrd service...&quot; sudo systemctl restart repmgrd log &quot;Follow script completed.&quot; else log &quot;Node is primary, but no mismatch in status detected. No action required.&quot; fifi Step 5: Setup replication clusterExecute on primary node (ilham-node1)Register the primary node 1sudo -u postgres repmgr -f /etc/repmgr/16/repmgr.conf primary register Verify registration: 1sudo -u postgres repmgr -f /etc/repmgr/16/repmgr.conf cluster show 123 ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+-------------+---------+-----------+-------------+----------+----------+----------+--------------------------------------------------------------- 1 | ilham-node1 | primary | * running | | default | 100 | 1 | host=192.168.0.21 user=repmgr dbname=repmgr connect_timeout=2 Execute on standby node (ilham-node2)Clone the standby node 1234567ls -lah /var/lib/postgresql/16/mainsystemctl stop postgresqlsystemctl status postgresqlcd /var/lib/postgresql/16/mainrm -rf ./*ls -lah /var/lib/postgresql/16/maincd ~ 1sudo -u postgres repmgr -h 10.70.200.18 -U repmgr -d repmgr -f /etc/repmgr/16/repmgr.conf standby clone --dry-run 1sudo -u postgres repmgr -h 10.70.200.18 -U repmgr -d repmgr -f /etc/repmgr/16/repmgr.conf standby clone 12systemctl start postgresqlsystemctl status postgresql Verify registration: 12sudo -u postgres psqlSELECT * FROM pg_stat_wal_receiver; Register standby node 1sudo -u postgres repmgr -f /etc/repmgr/16/repmgr.conf standby register Verify registration 1sudo -u postgres repmgr -f /etc/repmgr/16/repmgr.conf cluster show 1234 ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+-------------+---------+-----------+-------------+----------+----------+----------+--------------------------------------------------------------- 1 | ilham-node1 | primary | * running | | default | 100 | 1 | host=192.168.0.21 user=repmgr dbname=repmgr connect_timeout=2 2 | ilham-node2 | standby | running | ilham-node1 | default | 100 | 1 | host=192.168.0.22 user=repmgr dbname=repmgr connect_timeout=2 1sudo -u postgres repmgr -f /etc/repmgr/16/repmgr.conf cluster event 12345 Node ID | Name | Event | OK | Timestamp | Details---------+-------------+----------------------------+----+---------------------+-------------------------------------------------- 1 | ilham-node1 | repmgrd_start | t | 2024-06-06 13:39:09 | monitoring cluster primary &quot;ilham-node1&quot; (ID: 1) 2 | ilham-node2 | repmgrd_upstream_reconnect | t | 2024-06-06 13:39:07 | reconnected to upstream node after 10 seconds 1 | ilham-node1 | child_node_new_connect | t | 2024-06-06 13:38:16 | new standby &quot;ilham-node2&quot; (ID: 2) has connected Step 6: Setup Repmgrd Service w&#x2F; SystemdExecute on both nodeLocate the existing init script (if any): 1sudo find /etc/init.d/ -name &#x27;repmgrd*&#x27; Remove or disable the existing init script: 123sudo cp /etc/init.d/repmgrd /root/repmgrd.backupls -l /root/repmgrd.backupsudo rm /etc/init.d/repmgrd Create a configuration file to ensure the necessary directory is created with the correct permissions at boot: 1sudo vim /etc/tmpfiles.d/repmgrd.conf Add the following content to the file: 1d /run/repmgr 0755 postgres postgres - Create and edit the repmgrd service file: 12touch /var/log/repmgr/repmgr.log chown postgres:postgres /var/log/repmgr/repmgr.log 1sudo vim /etc/systemd/system/repmgrd.service 1234567891011121314151617181920212223[Unit]Description=Manage repmgrd for PostgreSQLAfter=network.target postgresql.serviceRequires=postgresql.service[Service]Type=forkingUser=postgresExecStart=/usr/bin/repmgrd -f /etc/repmgr/16/repmgr.conf -d --pid-file=/run/repmgr/repmgrd.pidPIDFile=/run/repmgr/repmgrd.pidExecStop=/bin/kill -TERM $MAINPIDExecReload=/bin/kill -HUP $MAINPIDKillMode=processRestart=on-failureRestartSec=5sStandardOutput=journalStandardError=journalSyslogIdentifier=repmgrdRuntimeDirectory=repmgrRuntimeDirectoryMode=0755[Install]WantedBy=multi-user.target Reload and restart service 1234sudo systemctl daemon-reloadsudo systemctl enable repmgrdsudo systemctl start repmgrdsudo systemctl status repmgrd Check repmgrd logs to ensure it’s running correctly 12sudo journalctl -u repmgrdsudo -u postgres repmgr cluster show Step 7: Configure keepalivedExecute on primary nodeConfigure file keepalived.conf 12sudo echo &#x27;&#x27; &gt; /var/log/update_keepalived_priority.logsudo cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.backup 1sudo vim /etc/keepalived/keepalived.conf 12345678910111213141516171819202122232425262728293031323334global_defs &#123; script_user root enable_script_security&#125;vrrp_script pg_check &#123; script &quot;/usr/local/bin/check_postgres.sh&quot; interval 5 timeout 5 rise 3 fall 3&#125;vrrp_instance VI_1 &#123; state MASTER interface ens3 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass repmgrpa &#125; virtual_ipaddress &#123; 192.168.0.50/24 dev ens3 &#125; track_script &#123; pg_check &#125; notify_backup &quot;/usr/local/bin/follow.sh&quot; notify_fault &quot;/usr/local/bin/follow.sh&quot;&#125; Create healtcheck script 12touch /usr/local/bin/check_postgres.shchmod +x /usr/local/bin/check_postgres.sh 1vim /usr/local/bin/check_postgres.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100#!/bin/bash# Configurationuser=&#x27;repmgr&#x27;database=&#x27;repmgr&#x27;logfile=&#x27;/var/log/check_postgres.log&#x27;current_node=&#x27;192.168.0.21&#x27;# Logging functionlog() &#123; echo &quot;$(date +&#x27;%Y-%m-%d %H:%M:%S&#x27;) - $1&quot; | tee -a $logfile&#125;log &quot;Checking PostgreSQL status on node $current_node...&quot;# Check if the node is in recovery mode (standby)is_standby=$(psql -h $current_node -U $user -d $database -tAc &quot;SELECT pg_is_in_recovery()&quot; 2&gt;&gt; $logfile)if [ $? -ne 0 ]; then log &quot;Error checking recovery mode on node $current_node.&quot; exit 1fiif [ &quot;$is_standby&quot; == &quot;t&quot; ]; then log &quot;Node $current_node is standby. Checking replication status.&quot; replication_status=$(psql -h $current_node -U $user -d $database -tAc &quot;SELECT pg_last_wal_receive_lsn() IS NOT NULL AND pg_last_wal_replay_lsn() IS NOT NULL&quot; 2&gt;&gt; $logfile) if [ $? -ne 0 ]; then log &quot;Error checking replication status on node $current_node.&quot; exit 1 fi if [ &quot;$replication_status&quot; == &quot;t&quot; ]; then log &quot;Replication is active on node $current_node. PostgreSQL is working properly.&quot; exit 0 else log &quot;Replication is not active on node $current_node. Failover should be initiated!&quot; exit 1 fielse log &quot;Node $current_node is primary. Performing create/drop table check.&quot; # Drop the test table if it exists psql -h $current_node -U $user -d $database -c &quot;DROP TABLE IF EXISTS test;&quot; 2&gt;&gt; $logfile if [ $? -ne 0 ]; then log &quot;Error dropping existing test table on node $current_node. Failover should be initiated!&quot; exit 1 fi # Create the test table psql -h $current_node -U $user -d $database -c &quot;CREATE TABLE test (id serial primary key, data varchar(255));&quot; 2&gt;&gt; $logfile if [ $? -ne 0 ]; then log &quot;Error creating table on node $current_node. Failover should be initiated!&quot; exit 1 fi table_check=$(psql -h $current_node -U $user -d $database -tAc &quot;SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = &#x27;test&#x27;);&quot; 2&gt;&gt; $logfile) if [ $? -ne 0 ]; then log &quot;Error checking table existence on node $current_node.&quot; exit 1 fi if [ &quot;$table_check&quot; == &quot;t&quot; ]; then log &quot;Table created successfully on node $current_node.&quot; # Drop the test table psql -h $current_node -U $user -d $database -c &quot;DROP TABLE test;&quot; 2&gt;&gt; $logfile if [ $? -ne 0 ]; then log &quot;Error dropping table on node $current_node. Failover should be initiated!&quot; exit 1 fi table_check=$(psql -h $current_node -U $user -d $database -tAc &quot;SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = &#x27;test&#x27;);&quot; 2&gt;&gt; $logfile) if [ $? -ne 0 ]; then log &quot;Error checking table existence after drop on node $current_node.&quot; exit 1 fi if [ &quot;$table_check&quot; == &quot;f&quot; ]; then log &quot;Table dropped successfully on node $current_node. PostgreSQL is working properly.&quot; exit 0 else log &quot;Table not dropped successfully on node $current_node. Failover should be initiated!&quot; exit 1 fi else log &quot;Error creating table on node $current_node. Failover should be initiated!&quot; exit 1 fifi Setup priority and state keepalived script 1vim /usr/local/bin/update_keepalived_priority.sh 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#!/bin/bash# Configurationuser=&#x27;repmgr&#x27;database=&#x27;repmgr&#x27;current_node=192.168.0.21config_file=&#x27;/etc/keepalived/keepalived.conf&#x27;primary_priority=100standby_priority=90log_file=&#x27;/var/log/update_keepalived_priority.log&#x27;# Logging functionlog() &#123; echo &quot;$(date +&#x27;%Y-%m-%d %H:%M:%S&#x27;) - $1&quot; | tee -a $log_file&#125;log &quot;Checking PostgreSQL role on node $current_node...&quot;# Check if the node is primaryis_primary=$(psql -h $current_node -U $user -d $database -tAc &quot;SELECT pg_is_in_recovery()&quot; 2&gt;&gt; $log_file)if [ $? -ne 0 ]; then log &quot;Error checking role of the node $current_node.&quot; exit 1fi# Determine current priority and state in keepalived.confcurrent_priority=$(grep -E &quot;^\\s*priority&quot; $config_file | awk &#x27;&#123;print $2&#125;&#x27;)current_state=$(grep -E &quot;^\\s*state&quot; $config_file | awk &#x27;&#123;print $2&#125;&#x27;)log &quot;Current priority: $current_priority&quot;log &quot;Current state: $current_state&quot;# Update the priority and state if they do not match the roleif [ &quot;$is_primary&quot; == &quot;f&quot; ]; then if [ &quot;$current_priority&quot; -ne $primary_priority ] || [ &quot;$current_state&quot; != &quot;MASTER&quot; ]; then log &quot;Node $current_node is primary. Updating priority to $primary_priority and state to MASTER.&quot; sed -i &quot;s/^\\s*priority.*/ priority $primary_priority/&quot; $config_file sed -i &quot;s/^\\s*state.*/ state MASTER/&quot; $config_file systemctl reload keepalived log &quot;Keepalived priority updated to $primary_priority and state to MASTER. Service reloaded.&quot; else log &quot;Node $current_node is primary and keepalived is already set to priority $primary_priority and state MASTER. No changes made.&quot; fielse if [ &quot;$current_priority&quot; -ne $standby_priority ] || [ &quot;$current_state&quot; != &quot;BACKUP&quot; ]; then log &quot;Node $current_node is standby. Updating priority to $standby_priority and state to BACKUP.&quot; sed -i &quot;s/^\\s*priority.*/ priority $standby_priority/&quot; $config_file sed -i &quot;s/^\\s*state.*/ state BACKUP/&quot; $config_file systemctl reload keepalived log &quot;Keepalived priority updated to $standby_priority and state to BACKUP. Service reloaded.&quot; else log &quot;Node $current_node is standby and keepalived is already set to priority $standby_priority and state BACKUP. No changes made.&quot; fifi add script to crontab 1crontab -e 1* * * * * /usr/local/bin/update_keepalived_priority.sh Restart service and verify 12sudo useradd -r keepalived_scriptkeepalived -t -f /etc/keepalived/keepalived.conf 123sudo systemctl enable keepalivedsudo systemctl restart keepalivedsudo systemctl status keepalived 1ip addr show ens3 Execute on standby nodeConfigure file keepalived.conf 12sudo echo &#x27;&#x27; &gt; /var/log/update_keepalived_priority.logsudo cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.backup 1sudo vim /etc/keepalived/keepalived.conf 12345678910111213141516171819202122232425262728293031323334global_defs &#123; script_user root enable_script_security&#125;vrrp_script pg_check &#123; script &quot;/usr/local/bin/check_postgres.sh&quot; interval 5 timeout 5 rise 3 fall 3&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens3 virtual_router_id 51 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass repmgrpa &#125; virtual_ipaddress &#123; 192.168.0.50/24 dev ens3 &#125; track_script &#123; pg_check &#125; notify_backup &quot;/usr/local/bin/follow.sh&quot; notify_fault &quot;/usr/local/bin/follow.sh&quot;&#125; Create healtcheck script 12touch /usr/local/bin/check_postgres.shchmod +x /usr/local/bin/check_postgres.sh 1vim /usr/local/bin/check_postgres.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100#!/bin/bash# Configurationuser=&#x27;repmgr&#x27;database=&#x27;repmgr&#x27;logfile=&#x27;/var/log/check_postgres.log&#x27;current_node=&#x27;192.168.0.22&#x27;# Logging functionlog() &#123; echo &quot;$(date +&#x27;%Y-%m-%d %H:%M:%S&#x27;) - $1&quot; | tee -a $logfile&#125;log &quot;Checking PostgreSQL status on node $current_node...&quot;# Check if the node is in recovery mode (standby)is_standby=$(psql -h $current_node -U $user -d $database -tAc &quot;SELECT pg_is_in_recovery()&quot; 2&gt;&gt; $logfile)if [ $? -ne 0 ]; then log &quot;Error checking recovery mode on node $current_node.&quot; exit 1fiif [ &quot;$is_standby&quot; == &quot;t&quot; ]; then log &quot;Node $current_node is standby. Checking replication status.&quot; replication_status=$(psql -h $current_node -U $user -d $database -tAc &quot;SELECT pg_last_wal_receive_lsn() IS NOT NULL AND pg_last_wal_replay_lsn() IS NOT NULL&quot; 2&gt;&gt; $logfile) if [ $? -ne 0 ]; then log &quot;Error checking replication status on node $current_node.&quot; exit 1 fi if [ &quot;$replication_status&quot; == &quot;t&quot; ]; then log &quot;Replication is active on node $current_node. PostgreSQL is working properly.&quot; exit 0 else log &quot;Replication is not active on node $current_node. Failover should be initiated!&quot; exit 1 fielse log &quot;Node $current_node is primary. Performing create/drop table check.&quot; # Drop the test table if it exists psql -h $current_node -U $user -d $database -c &quot;DROP TABLE IF EXISTS test;&quot; 2&gt;&gt; $logfile if [ $? -ne 0 ]; then log &quot;Error dropping existing test table on node $current_node. Failover should be initiated!&quot; exit 1 fi # Create the test table psql -h $current_node -U $user -d $database -c &quot;CREATE TABLE test (id serial primary key, data varchar(255));&quot; 2&gt;&gt; $logfile if [ $? -ne 0 ]; then log &quot;Error creating table on node $current_node. Failover should be initiated!&quot; exit 1 fi table_check=$(psql -h $current_node -U $user -d $database -tAc &quot;SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = &#x27;test&#x27;);&quot; 2&gt;&gt; $logfile) if [ $? -ne 0 ]; then log &quot;Error checking table existence on node $current_node.&quot; exit 1 fi if [ &quot;$table_check&quot; == &quot;t&quot; ]; then log &quot;Table created successfully on node $current_node.&quot; # Drop the test table psql -h $current_node -U $user -d $database -c &quot;DROP TABLE test;&quot; 2&gt;&gt; $logfile if [ $? -ne 0 ]; then log &quot;Error dropping table on node $current_node. Failover should be initiated!&quot; exit 1 fi table_check=$(psql -h $current_node -U $user -d $database -tAc &quot;SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = &#x27;test&#x27;);&quot; 2&gt;&gt; $logfile) if [ $? -ne 0 ]; then log &quot;Error checking table existence after drop on node $current_node.&quot; exit 1 fi if [ &quot;$table_check&quot; == &quot;f&quot; ]; then log &quot;Table dropped successfully on node $current_node. PostgreSQL is working properly.&quot; exit 0 else log &quot;Table not dropped successfully on node $current_node. Failover should be initiated!&quot; exit 1 fi else log &quot;Error creating table on node $current_node. Failover should be initiated!&quot; exit 1 fifi Setup priority and state keepalived script 1vim /usr/local/bin/update_keepalived_priority.sh 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#!/bin/bash# Configurationuser=&#x27;repmgr&#x27;database=&#x27;repmgr&#x27;current_node=192.168.0.22config_file=&#x27;/etc/keepalived/keepalived.conf&#x27;primary_priority=100standby_priority=90log_file=&#x27;/var/log/update_keepalived_priority.log&#x27;# Logging functionlog() &#123; echo &quot;$(date +&#x27;%Y-%m-%d %H:%M:%S&#x27;) - $1&quot; | tee -a $log_file&#125;log &quot;Checking PostgreSQL role on node $current_node...&quot;# Check if the node is primaryis_primary=$(psql -h $current_node -U $user -d $database -tAc &quot;SELECT pg_is_in_recovery()&quot; 2&gt;&gt; $log_file)if [ $? -ne 0 ]; then log &quot;Error checking role of the node $current_node.&quot; exit 1fi# Determine current priority and state in keepalived.confcurrent_priority=$(grep -E &quot;^\\s*priority&quot; $config_file | awk &#x27;&#123;print $2&#125;&#x27;)current_state=$(grep -E &quot;^\\s*state&quot; $config_file | awk &#x27;&#123;print $2&#125;&#x27;)log &quot;Current priority: $current_priority&quot;log &quot;Current state: $current_state&quot;# Update the priority and state if they do not match the roleif [ &quot;$is_primary&quot; == &quot;f&quot; ]; then if [ &quot;$current_priority&quot; -ne $primary_priority ] || [ &quot;$current_state&quot; != &quot;MASTER&quot; ]; then log &quot;Node $current_node is primary. Updating priority to $primary_priority and state to MASTER.&quot; sed -i &quot;s/^\\s*priority.*/ priority $primary_priority/&quot; $config_file sed -i &quot;s/^\\s*state.*/ state MASTER/&quot; $config_file systemctl reload keepalived log &quot;Keepalived priority updated to $primary_priority and state to MASTER. Service reloaded.&quot; else log &quot;Node $current_node is primary and keepalived is already set to priority $primary_priority and state MASTER. No changes made.&quot; fielse if [ &quot;$current_priority&quot; -ne $standby_priority ] || [ &quot;$current_state&quot; != &quot;BACKUP&quot; ]; then log &quot;Node $current_node is standby. Updating priority to $standby_priority and state to BACKUP.&quot; sed -i &quot;s/^\\s*priority.*/ priority $standby_priority/&quot; $config_file sed -i &quot;s/^\\s*state.*/ state BACKUP/&quot; $config_file systemctl reload keepalived log &quot;Keepalived priority updated to $standby_priority and state to BACKUP. Service reloaded.&quot; else log &quot;Node $current_node is standby and keepalived is already set to priority $standby_priority and state BACKUP. No changes made.&quot; fifi add script to crontab 1crontab -e 1* * * * * /usr/local/bin/update_keepalived_priority.sh Restart service and verify 12sudo useradd -r keepalived_scriptkeepalived -t -f /etc/keepalived/keepalived.conf 123sudo systemctl enable keepalivedsudo systemctl restart keepalivedsudo systemctl status keepalived 1ip addr show ens3 Step 8: Verify the SetupCluster Status Verification 12sudo -u postgres repmgr -f /etc/repmgr/16/repmgr.conf cluster showip addr show ens3 Check the status of keepalived: 1sudo systemctl status keepalived HA TESTING1. Checking Cluster StatusOn ilham-node1 (primary node)Cluster Status Verification 12date; sudo -u postgres repmgr -f /etc/repmgr/16/repmgr.conf cluster showdate; ip addr show ens3 2. Simulate FailoverStop PostgreSQL on the Primary Node 12date; sudo systemctl stop postgresqldate; sudo systemctl status postgresql Check Logs for Failover Events at Standby node: 1tail -f /var/log/repmgr/repmgr.log 3. Post-Failover Cluster Status CheckAt standby nodeCluster Status Verification 12date; sudo -u postgres repmgr -f /etc/repmgr/16/repmgr.conf cluster showdate; ip addr show ens3 Monitor keepalived logs to track the state transitions and ensure the node is correctly managed as a backup. 1tail -f /var/log/syslog | grep -i keepalived 4. Reintegration of Failure Node into the Cluster Check vrrp ip at failure node 1date; ip addr show ens3 Start the PostgreSQL service on ilham-node1 (Failure Node) and verify its status to reintegrate the node into the cluster. 12date; sudo systemctl start postgresql; sleep 2; sudo systemctl status postgresqldate; ip addr show ens3 Check keepalived log at ilham-node1 1tail -f /var/log/syslog | grep -i keepalived The transitions from FAULT STATE to BACKUP STATE show that keepalived successfully manages the state of the node, ensuring it is ready to serve as a standby in the high availability setup.","categories":[{"name":"database","slug":"database","permalink":"https://ilosaur.us/categories/database/"}],"tags":[{"name":"database","slug":"database","permalink":"https://ilosaur.us/tags/database/"},{"name":"postgresql","slug":"postgresql","permalink":"https://ilosaur.us/tags/postgresql/"},{"name":"replication","slug":"replication","permalink":"https://ilosaur.us/tags/replication/"},{"name":"repmgr","slug":"repmgr","permalink":"https://ilosaur.us/tags/repmgr/"},{"name":"keepalived","slug":"keepalived","permalink":"https://ilosaur.us/tags/keepalived/"}],"keywords":[{"name":"database","slug":"database","permalink":"https://ilosaur.us/categories/database/"}]},{"title":"EFK Stack - Parsing Log with Fluentd","slug":"efk-stack-fluentd-parsing","date":"2020-04-13T09:03:00.000Z","updated":"2020-04-13T09:03:28.000Z","comments":true,"path":"2020/04/13/efk-stack-fluentd-parsing/","permalink":"https://ilosaur.us/2020/04/13/efk-stack-fluentd-parsing/","excerpt":"In this session I will share about something that is quite important in the production environment, namely logging. Logs help us in analyzing and identifying the conditions of an application. The challenge is when we try to identify logs in each application on a different server or location that effort or spend a lot of time. The solution is by applying a Log Server so that the log in each application can be centralized.","text":"In this session I will share about something that is quite important in the production environment, namely logging. Logs help us in analyzing and identifying the conditions of an application. The challenge is when we try to identify logs in each application on a different server or location that effort or spend a lot of time. The solution is by applying a Log Server so that the log in each application can be centralized. The next challenge is how the centralized log is Human readable or can be identified more easily by humans to provide stunning insight or summary. One of them is by parsing raw logs into smaller pieces. In this session I will share how to : Parsing simple raw log Parsing json raw log Parsing raw log xml Parsing multiline log Then the stack is used : CentOS 8 as Operating System. Elasticsearch as log storage. Fluentd as a log aggregation, parser, and agent. Kibana as a visualize tool integrated with Elasticsearch. For details on the EFK stack deploying process, please read the following url https://ilosaur.us/2020/04/11/efk-stack-deploy-efk-stack/ To do a parsing log on fluentd, I use the parser plugin provided by fluentd, including regexp, apache2, apache_error, nginx, syslog, csv, tsv, ltsv, json, msgpack, multiline, none and third party parser grok. Simple parse raw log using fluentd regexp parser.Lets start running python apps to generate dummy logs. 12345678910import datetime, time, random, stringdef randomString(stringLength=10): letters = string.ascii_uppercase return &#x27;&#x27;.join(random.choice(letters) for i in range(stringLength))while True: time.sleep(20) print(datetime.datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)+&quot; - SIMPLEAPPS ORDER-&quot;+randomString(14)+&quot; &quot;+str(random.randrange(10,99,1))) 12345mkdir /var/log/simplepython3 simple-python.py &gt;&gt; /var/log/simple/python-simple.log``` And we can tail this log file [root@efk-fluentd simple]# pwd&#x2F;var&#x2F;log&#x2F;simple[root@efk-fluentd simple]# tree.├── python-simple.log├── python-simple.py0 directories, 4 files[root@efk-fluentd simple]# python3 python-simple.py &gt;&gt; &#x2F;var&#x2F;log&#x2F;simple&#x2F;python-simple.log &amp;[1] 8362[root@efk-fluentd simple]# tail -f &#x2F;var&#x2F;log&#x2F;simple&#x2F;python-simple.log2020-04-13 20:39:48 - SIMPLEAPPS ORDER-NNZWQAIAGQCVJM 152020-04-13 20:40:18 - SIMPLEAPPS ORDER-YTHWQVIKHZUSFE 462020-04-13 20:40:48 - SIMPLEAPPS ORDER-JWTDMRKLONUDNT 542020-04-13 20:41:18 - SIMPLEAPPS ORDER-JKDSUVPFLUTPZW 67 123Edit file `/etc/td-agent/td-agent.conf` , add line `@include config.d/*.conf` for directives in separate configuration files. vi &#x2F;etc&#x2F;td-agent&#x2F;td-agent.confmkdir &#x2F;etc&#x2F;td-agent&#x2F;config.d 123Create config file for parsing string log from file `/var/log/simple/python-simple.log.` vi &#x2F;etc&#x2F;td-agent&#x2F;config.d&#x2F;simple-parsing.confsystemctl restart td-agentsystemctl status td-agent 1 @type tail @id python_simple @type regexp expression (?[^ ]* [^ ]*) - (?[^ ]*) (?[^ ]*) (?[^ ]*) time_key logtime time_format %Y-%m-%d %H:%M:%S types count:integer path /var/log/simple/python-simple.log tag simple_parsing @type stdout 1 #2020-04-13 21:05:49 - SIMPLEAPPS ORDER-EUJPYYAYEOJQKW 92 expression (?[^ ]* [^ ]*) - (?[^ ]*) (?[^ ]*) (?[^ ]*) 1234In config file simple-parsing.log using plugin parser regexp to parse raw log from /var/log/simple/python-simple.log. And in match section using output plugin stdout so logs from file /var/log/simple/python-simple.log will be forwarded to /var/log/td-agent/td-agent.log as stdout logs.Now, just tail file /var/log/td-agent/td-agent. log to check the parsing result. 2020-04-13 21:05:49.000000000 +0700 simple_parsing: {\"apps-name\":\"SIMPLEAPPS\",\"invoice\":\"ORDER-EUJPYYAYEOJQKW\",\"count\":92} 2020-04-13 21:06:19.000000000 +0700 simple_parsing: {\"apps-name\":\"SIMPLEAPPS\",\"invoice\":\"ORDER-KRCSOOQVYGTKMW\",\"count\":68} 2020-04-13 21:07:19.000000000 +0700 simple_parsing: {\"apps-name\":\"SIMPLEAPPS\",\"invoice\":\"ORDER-EXKBWKNDKLXGZS\",\"count\":61} 1 time: 2020-04-13 21:05:49.000000000 +0700 simple_parsing: record: { \"apps-name\":\"SIMPLEAPPS\", \"invoice\":\"ORDER-EUJPYYAYEOJQ\", \"count\":\"92\" } 1234## Simple parse raw json log using fluentd json parser. In this section, we will parsing raw json log using fluentd json parser and output to stdout. Ok lets start with create generator log using simple python script. import datetime, sys, time, random, string def randomString(stringLength&#x3D;10): letters &#x3D; string.ascii_uppercase return ‘’.join(random.choice(letters) for i in range(stringLength)) while True: time.sleep(30) print(‘{“order_time”:”‘+datetime.datetime.now().strftime(“%Y-%m-%d %H:%M:%S”)+’”,”order_id”:”ORDER-‘+randomString(14)+’”,”order_amount”:”‘+str(random.randrange(10,99,1)*1000)+’”}’, flush&#x3D;True)","categories":[{"name":"logging","slug":"logging","permalink":"https://ilosaur.us/categories/logging/"}],"tags":[{"name":"efk","slug":"efk","permalink":"https://ilosaur.us/tags/efk/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://ilosaur.us/tags/elasticsearch/"},{"name":"logging","slug":"logging","permalink":"https://ilosaur.us/tags/logging/"},{"name":"fluentd","slug":"fluentd","permalink":"https://ilosaur.us/tags/fluentd/"},{"name":"parsing","slug":"parsing","permalink":"https://ilosaur.us/tags/parsing/"}],"keywords":[{"name":"logging","slug":"logging","permalink":"https://ilosaur.us/categories/logging/"}]},{"title":"EFK Stack - Deploying EFK Stack (Part1)","slug":"efk-stack-deploy-efk-stack","date":"2020-04-12T09:03:28.084Z","updated":"2020-04-12T09:03:28.092Z","comments":true,"path":"2020/04/12/efk-stack-deploy-efk-stack/","permalink":"https://ilosaur.us/2020/04/12/efk-stack-deploy-efk-stack/","excerpt":"Elasticsearch provides real-time search and analytics for all types of data. Whether you have structured or unstructured text, numerical data, or geospatial data, Elasticsearch can efficiently store and index it in a way that supports fast searches. You can go far beyond simple data retrieval and aggregate information to discover trends and patterns in your data. And as your data and query volume grows, the distributed nature of Elasticsearch enables your deployment to grow seamlessly right along with it.","text":"Elasticsearch provides real-time search and analytics for all types of data. Whether you have structured or unstructured text, numerical data, or geospatial data, Elasticsearch can efficiently store and index it in a way that supports fast searches. You can go far beyond simple data retrieval and aggregate information to discover trends and patterns in your data. And as your data and query volume grows, the distributed nature of Elasticsearch enables your deployment to grow seamlessly right along with it. Elasticsearch is commonly deployed alongside Kibana, a powerful data visualization and dashboard for Elasticsearch. Kibana allows you to explore your Elasticsearch log data through a web interface, and build dashboards and queries to quickly answer questions and gain insight into your Kubernetes applications. EFK Stack Elasticsearch Version 7.6 Fluentd Version 2.6 Kibana Version 7.6 Install ElasticsearchInstall Elasticsearch on node efk-elastic-kibana Update &amp; Upgrade packages1234yum -y updateyum -y upgradeyum -y install epel-releasereboot Set Timezone1timedatectl set-timezone Asia/Jakarta Install &amp; Start Chronyd123yum -y install chronysystemctl enable --now chronydsystemctl status chronyd Install OpenJDK1yum install java-1.8.0-openjdk Add repository Elasticsearch123456789101112131415cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/elasticsearch.repo[elasticsearch-7.x]name=Elasticsearch repository for 7.x packagesbaseurl=https://artifacts.elastic.co/packages/7.x/yumgpgcheck=1gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearchenabled=1autorefresh=1type=rpm-mdEOFrpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearchyum clean allyum makecacheyum repolist Install Elasticsearch1yum -y install elasticsearch Config Elasticsearch, edit file /etc/elasticsearch/elasticsearch.yml1cp /etc/elasticsearch/elasticsearch.yml /etc/elasticsearch/elasticsearch.yml.original 12345678vi /etc/elasticsearch/elasticsearch.ymlnode.name: efk-elastic-kibanadiscovery.zen.minimum_master_nodes: 1network.host: 0.0.0.0http.port: 9200discovery.seed_hosts: [&quot;127.0.0.1&quot;]cluster.initial_master_nodes: [&quot;efk-elastic-kibana&quot;] Enable &amp; Start Elasticsearch Service123systemctl daemon-reloadsystemctl enable --now elasticsearchsystemctl status elasticsearch Verify Elasticsearch123netstat -tulpn | grep 9200curl http://127.0.0.1:9200/_cluster/health?prettycurl http://127.0.0.1:9200/_cat/nodes?v Example Output 123456789101112131415161718192021222324[root@efk-elastic-kibana ~]# curl http://10.199.199.123:9200/_cluster/health?pretty&#123; &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;status&quot; : &quot;green&quot;, &quot;timed_out&quot; : false, &quot;number_of_nodes&quot; : 1, &quot;number_of_data_nodes&quot; : 1, &quot;active_primary_shards&quot; : 6, &quot;active_shards&quot; : 6, &quot;relocating_shards&quot; : 0, &quot;initializing_shards&quot; : 0, &quot;unassigned_shards&quot; : 3, &quot;delayed_unassigned_shards&quot; : 0, &quot;number_of_pending_tasks&quot; : 0, &quot;number_of_in_flight_fetch&quot; : 0, &quot;task_max_waiting_in_queue_millis&quot; : 0, &quot;active_shards_percent_as_number&quot; : 66.66666666666666&#125;[root@efk-elastic-kibana ~]# curl http://10.199.199.123:9200/_cat/indices?vhealth status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open .kibana_task_manager_1 c2A6LOb9QQySpcvfY2r_RQ 1 0 2 1 29.8kb 29.8kbgreen open .apm-agent-configuration EM_3qi6yQOmP9unMRtFJ2g 1 0 0 0 283b 283bgreen open .kibana_1 bsbGNLfDTdqjALyJwBCkIQ 1 0 9 1 44.7kb 44.7kb[root@efk-elastic-kibana ~]# Install KibanaInstall Kibana on node efk-elastic-kibana Install kibana1yum -y install kibana Config Kibana, edit file /etc/kibana/kibana.yml1cp /etc/kibana/kibana.yml /etc/kibana/kibana.yml.original 123vi /etc/kibana/kibana.ymlelasticsearch.hosts: [&quot;http://localhost:9200&quot;] Enable &amp; Start Kibana12systemctl enable --now kibanasystemctl status kibana Install nginx as reverse proxy1yum -y install nginx apache2-utils Generate credential for basic http auth12# kibana passwordecho &quot;kibana:`openssl passwd -apr1`&quot; | sudo tee -a /etc/nginx/htpasswd.kibana Config nginx, edit file /etc/nginx/nginx.conf1cp /etc/nginx/nginx.conf /etc/nginx/nginx.conf.original 123456789101112131415161718192021222324252627282930313233343536373839404142vi /etc/nginx/nginx.confuser nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;include /usr/share/nginx/modules/*.conf;events &#123; worker_connections 1024;&#125;http &#123; log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; include /etc/nginx/conf.d/*.conf; server &#123; listen 80; server_name _; auth_basic &quot;Restricted Access&quot;; auth_basic_user_file /etc/nginx/htpasswd.kibana; location / &#123; proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &#x27;upgrade&#x27;; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; &#125; &#125; &#125; Verify config nginx1nginx -t Enable &amp; Restart nginx123systemctl enable nginx systemctl restart nginxsystemctl status nginx Access kibana via browser Next chapter :Introduction, Install &amp; parsing simple log on fluentdParsing Apps Logs from Docker ContainerEFK Stack Integration with Kubernetes CLuster","categories":[{"name":"logging","slug":"logging","permalink":"https://ilosaur.us/categories/logging/"}],"tags":[{"name":"efk","slug":"efk","permalink":"https://ilosaur.us/tags/efk/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://ilosaur.us/tags/elasticsearch/"},{"name":"logging","slug":"logging","permalink":"https://ilosaur.us/tags/logging/"},{"name":"fluentd","slug":"fluentd","permalink":"https://ilosaur.us/tags/fluentd/"}],"keywords":[{"name":"logging","slug":"logging","permalink":"https://ilosaur.us/categories/logging/"}]}]}